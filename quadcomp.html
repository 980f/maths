<html>
  <head>
    <meta content="text/html; charset=windows-1252" http-equiv="content-type">
  </head>
  <body>(C) 1992 by Andy Heilveil.
    <br>
    Derivation of integral preserving quadratic compression: <br>
    First I shall describe how smooth linear compression works since the quadratic version is a trivial extension. <br>
    We are merging
    a larger number of input points to create a smaller number of
    output points. <br>
    Viewed graphically we are integrating over a
    spread of X coordinates to generate a Y datum. <br>
    Each input datum
    represents a fractional Y datum, I call this fraction XSTEP. <br>
    The
    heart of the algorithm is to add the next X datum to the
    accumulating Y datum, <br>
    add XSTEP to YFRAC where YFRAC is how much
    of the range of the Y datum has been accumulated. <br>
    If XSTEP is 1/
    an integer then that number of whole X data go into a Y datum. As
    this is often not the case we encounter X datums that need to be
    split between two Y datums. (I use the word 'datums' to refer to
    a small subset of a block of data). <br>
    To know how much of the X to
    go into each Y we inspect the YFRAC after each increment by
    XSTEP. <br>
    If YFRAC is &gt;1 then we have crossed into the new Y datum,
    and the fractional part of YFRAC tells us how much of the X datum
    belongs to the NEW point. This is also the amount that was added
    erroneously to the Y datum-in-process so we calculate the
    fraction of X, subtract it from present accumulation, output
    accumulator, load accumulator with fraction-of-X. <br>
    To know when to
    do this we have watched YFRAC become greater than 1. When it does so
    we thereafter only care about the fractional part of it so we can
    subtract one each time it happens. <br>
    When programming at assembly
    level or building hardware we would let the carry bit of the
    addition be the ONE's position of YFRAC which automatically gets
    lost when the next add is done. <br>
    To extend this algorithm for quadratic compression we do no
    more than modify XSTEP with each data point. The modification is
    to subtract a small amount A from XSTEP either each cycle of X or
    for faster operation at slightly less smooth compression we can
    do it only after outputting each Y datum. Changing XSTEP is in
    effect altering the compression ratio with each step. <br>
    <br>
    To
    calculate what A should be we need the following input:
    <br>
    DX: number of input points to compress,
    <br>
    DY: number of output points OR equivalently
    <br>
    NR: net compression desired= DY/DX, a number &lt; one.
    <br>
    We also need:
    <br>
    IR: initial compression rate= starting value for XSTEP. The free choice of IR corresponds to choosing what part of
    the quadratic curve we are using to compress by. A common sense
    IR value is ONE saying that we want the low end data to be
    approximately 1:1 as there is no advantage to expanding data. As an aside: a slight modification of the loop, which can in fact
    be coded as a common loop, provides expansion rather than compression.
    <br>
    An IR value less than one has the effect of losing resolution at
    the low end to gain resolution (lesser local compression) at the
    high end. <br>
    The NR actual may be slightly different then the NR desired
    due to round off of computer math. <br>
    NR can usually be allowed to
    wander enough so that the A parameter, whose LSB determines the
    precision of YFRAC, XSTEP, and itself, is in range of fixed point
    math. Without this pragmatism the algorithm is somewhat
    computationally intensive. <br>
    Due to the way X datums are split the
    precision of the product YFRAC*X value can be quite limited, the
    integral is preserved up to an uncertainty of a fraction of the
    integration boundary coordinate. <br>
    DY= sum of XSTEP for Y steps. <br>
    XSTEP(0)= IR. <br>
    XSTEP(1)=IR-A; <br>
    XSTEP(2)=XSTEP(1)-A=IR-A-A=IR-A*2; <br>
    XSTEP(i)=IR-A*i; <br>
    sum of XSTEP(i) for i=0 to DX-1= IR*DX - A*(DX/2)*(DX-1) using the formula for sum of i. <br>
    DY= DX*[IR-(A/2)*(DX-1)]; notice this is like y=ax+bxý <br>
    NR= IR-(A/2)*(DX-1) <br>
    FR=final rate=IR-A*DX is approx. 2*NR which corresponds to the differential formula: dy/dx=d/dx(xý)=2*x.
    <br>
    A slight alteration of the order of operations in the
    processing loop changes the summation range to from 1 to DX which
    changes the formula for NR to: NR= IR-(A/2)*(DX+1), with DX &gt;&gt;1 we don't have to be picky
    about what we are doing. <br>
    So: A= 2*(IR-NR)/(DX-1). (or DX+1 if you prefer). <br>
    Note that IR must be greater than NR. If XSTEP goes below
    zero due to round off (or iterating cycle too many steps) the
    algorithm fails. <br>
    XSTEP exactly zero corresponds to an infinite
    compression ratio of 1/0, below zero this is the other branch of
    the 1/x hyperbola and as such yields expansion rather than
    compression. <br>
    Using DY instead of NR as a given, and choosing IR= ONE: A= 2*(1-DY/DX)/(DX-1) is approx. 2*(DX-DY)/DXý. <br>
    This last
    expression may be interpreted as "the A parameter is like twice
    the number of points to be discarded divided by the square of the
    number to keep". <br>
    For my favorite case of 4096 points compressing
    to 1024 (2 to 12th squeezed into 2 to 10th): <br>
    A=2*(1- (1024/4096))/(4095)= (3/2)/4095. <br>
    Multiplying by 4095/4096 creates a error 1/4095 every 4096th
    cycle of operation. This means no error until the last cycle
    whose result is trashed anyway. <br>
    This makes A=3*2^-13 which is
    1.1*2^-12 in base 2 which is .0000 0000 0001 1... which fits
    nicely into a 16 bit integer. <br>
    The infinitely precise number would
    be .0000 0000 0001 1000 0000 0001 1000 0000 0001 etc. repeating
    in cycles of 12 bits. <br>
    Other power of two ratios also nicely
    reduce to simple bit patterns, and similarly so for IRs that are
    a (negative) power of two. <br>
    In general one may truncate the A
    value to double the order of magnitude of the number of given
    points, e.g. 24 bit math for up to 4096 input points. If you record the actual net compression and use it for
    calculating offsets for integration then the error in A will be
    significantly compensated for.
    <hr>Higher order compressions, e.g. cubic, are implemented by
    modifying the A parameter each step. Any finite polynomial
    compression of order N can be reduced to N+1 adds, in effect
    integrating a constant N times to get a*x^N. One gets polynomial
    equations for the A terms which are in the limit of large numbers
    the Taylor series coefficients. I have never found a use for any
    such compression.
    <hr>The compression may be expressed in words: <br>
    accumulate X
    points until they equal a Y point. <br>
    Expansion is the complementary
    operation and can be expressed: <br>
    output fractions of the input
    point until it is exhausted. The fraction increases linearly with each
    output to make it quadratic. <br>
    For each X datum: <br>
    Do <br>
    &nbsp;&nbsp;&nbsp; output XSTEP* X datum <br>
    &nbsp;&nbsp;&nbsp; YFRAC +=XSTEP XSTEP -= A <br>
    until YFRAC&gt;=1 <br>
    YFRAC-=1 (retain just the fractional part) <br>
    pick up a new X datum. <br>
    <br>
    The above skips the fine detail of splitting X data via YFRAC
    at the boundaries. <br>
    DY= sum 1 to DY of XSTEP(i); <br>
    XSTEP(0)=IR <br>
    XSTEP(i)=XSTEP(i-1)-A; <br>
    XSTEP(i)=IR-A*i <br>
    DY= IR*DY-A*(DY/2)*(DY+1) <br>
    DY= DY*[IR-(A/2)*(DY+1)] <br>
    1 = IR-(A/2)*(DY+1) <br>
    A = 2*(1-IR)/(DY+1). <br>
    But this time around we can't choose IR to be one! So we have
    to add some other condition
  </body>
</html>
