<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <title>roundinghalves</title>
    <meta name="author" content="Andy Heilveil">
  </head>
  <body>
    <p>Banker's Rounding<br>
      aka Round to Even.</p>
    <p><br>
      When rounding numbers some people make a special case for values that are
      "perfectly between" two integers. </p>
    <p>Numbers in a computer are in fact never known to be 'perfectly' between
      two integers as they have a finite number of bits and as such have already
      been rounded, just to one part in 16 million for 32 bit IEEE numbers, one
      part in 2^52 for double precision IEEE.<br>
      As such there has already been a bit of truncation and some of those
      'perfectly betweens' were really slightly less and some slightly more than
      0.5, in the general case.</p>
    <p>In order to have the sum over many such numbers have an average of zero
      for those dropped bits one must randomly with 50/50 distribution round up
      and down. True randomness may not be available, and complicates testing of
      software so psuedo-random selection is common. One of those psuedo-random
      methods is to look at the LSB of the integer and add 1 or -1 according to
      it. This is called "round to even" and is cheaply implemented (once the
      items needing this consideration have been found) by clearing the lsb of
      the value used for non perfect halves.</p>
    <p>For this to actually be random the lsbs of the rounded numbers must be
      random. But even so it creates a correlation between 'rounding noise' and
      the 'signal' so is unsuited for things like audio processing. The latter
      only averages across near neighbors and as such the sample size is too
      small to average out the noise so there is no point in trying.<br>
      <br>
      <br>
      <br>
      Summarizing:<br>
      Finessing the rounding of values ending in 0.5 is done to average out the
      truncation errors made while binning data to some finite decimal. <br>
      <br>
      It only makes a difference when the rounded values are then fed into a
      process which accumulates over very many of them, so that there are enough
      actual samples of randomness to average to zero.</p>
    <p>If data is not going to be post processed then simple rounding is
      perfectly appropriate and will yield equal sized bins in any binning
      process.</p>
    <p>If the data fed into the original process isn't randomly distributed then
      'round to even' might not yield the added randomness intended for it. For
      example, a linear sequence with fractional step is at best evenly
      distributed across possible fractions. If small then it fails the test of
      random bit loss.</p>
  </body>
</html>
